{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 키워드 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7VLF8ci9xpw",
        "outputId": "8cd7b641-8fa0-4966-97b7-43bfc63760ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothic.ttf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumGothicExtraBold.ttf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicLight.otf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicLight.ttf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicUltraLight.otf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicUltraLight.ttf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicBold.ttf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothic.otf',\n",
              " 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicBold.otf']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "\n",
        "sys_font = fm.findSystemFonts()\n",
        "\n",
        "[f for f in sys_font if 'Nanum' in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtifkHICMA0o",
        "outputId": "90ea0c20-6baf-4cb6-d787-c547c543bdd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\koreacamel\\AppData\\Local\\Temp\\ipykernel_15736\\312527451.py:4: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-white')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "article_df= pd.read_csv(\"data/article_2000.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WsXwFDd2wk9v"
      },
      "outputs": [],
      "source": [
        "# 필요한 컬럼 리스트만들기\n",
        "\n",
        "#언론사 이름  - 불용어 처리를 위해서\n",
        "press_list = list(article_df.press)\n",
        "\n",
        "#기사 본문 \n",
        "articles_main = list(article_df.Article)\n",
        "\n",
        "#기사 제목\n",
        "articles_title = list(article_df.Title)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nzFTWZw8AFt_"
      },
      "source": [
        "### 전처리 \n",
        "- 불용어 사전 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#불용어 사전 가지고 오기 \n",
        "f = open(\"data/koreanStopwords.txt\", 'r', encoding = 'utf-8')\n",
        "lines = f.readlines()\n",
        "stopwords_korean = []\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    stopwords_korean.append(line)\n",
        "f.close()\n",
        "\n",
        "#결과물 확인하며 반복으로 추가 한다. \n",
        "stop_add1 = '삼성전자 베스트 수량 추천 쿠폰 파격 최초 공개 삼성 갤럭시 구매 플랫 폼 네이버 시 뉴스 진행 롯데 티몬 지난해 이번 쇼 세트 카카오 오후 관계자 플 번가 제품 소개 브랜드 현대홈쇼핑 배 온스타일 닷컴 공영 '\n",
        "stop_add1 = stop_add1.split(' ')\n",
        "\n",
        "stop_add2 = '특가 할인 세일 역대 타임 한정 시즌 사은품 무배 마지막 데일리 준비 인기 최저 기념 기획 코코 베베 위크 단독 추가 필수 신상 최대 특집 혜택 매트 증정 컬러 이벤트 데이 특별 무료 본사 모음 출시 프리미엄'\n",
        "stop_add2 = stop_add2.split(' ')\n",
        "\n",
        "\n",
        "stop_add3 = '라방 라이브 방송 전 커머스 쇼핑 상품 등 고객 판매 홈 난 일 걸 뭐 줄 만 건 분 개 끝 잼 이거 번 중 듯 때 게 내 말 나 수 거 점 것 제일제당 기사 기자 라이브'\n",
        "stop_add3 = stop_add3.split(' ')\n",
        "\n",
        "\n",
        "stop_words = stopwords_korean + stop_add1 +stop_add2 + stop_add3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 기사 본문, 제목에서 분석에 필요없는 단어 제거하기 위해서 정규식으로 한글만 남긴다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kyn4Zp5EAFiQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "articles_main_clean = [ ]\n",
        "\n",
        "for article in articles_main:\n",
        "  clean = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \" \", article)\n",
        "  articles_main_clean.append(clean)\n",
        "\n",
        "articles_title_clean = [ ]\n",
        "\n",
        "for title in articles_title:\n",
        "  clean = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \" \", title)\n",
        "  articles_title_clean.append(clean)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yzzihBV16w7R"
      },
      "source": [
        "### 데이터 추출 \n",
        "- 텍스트 분석을 위해 KoNLPy(“코엔엘파이”)는 한국어 정보처리를 위한 파이썬 패키지활용\n",
        "- Mecab 으로 형태소 분석하여 길이가 1이상인 일반명사, 고유명사, 형용사, 어근만 추출한다. \n",
        "- 불용어사전을 통해서 전처리 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aF7Ff7OSNVAf"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
        "\n",
        "nouns_article = [ ]\n",
        "for article in articles_main_clean:\n",
        "  tagged = tagger.pos(article)\n",
        "  nouns = [s for s, t in tagged if t in ['NNG', 'NNP', 'VA', 'XR'] and len(s) >1]\n",
        "  for noun in nouns:\n",
        "    if noun not in stop_words:\n",
        "        nouns_article.append(noun)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 파이썬 Counter 패키지를 통해 추출한 단어의 빈도수를 측정한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N5kLIKwSAPv",
        "outputId": "be0d09d0-cd3f-4d33-e7e2-5d566147247f"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "nouns_counter = Counter(nouns_article)\n",
        "\n",
        "#상위 200개 데이터\n",
        "top_nouns_article = dict(nouns_counter.most_common(200))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 데이터 시각화"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 빈도수 상위 200개 데이터로 워드클라우드 생성 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubNKzzSqSAJh",
        "outputId": "8650f144-28c8-4873-c1e0-6336bd8ae1ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<wordcloud.wordcloud.WordCloud at 0x2b30ab4a820>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(background_color = 'white', font_path = 'C:\\\\Users\\\\koreacamel\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothicBold.otf')\n",
        "\n",
        "wc.generate_from_frequencies(top_nouns_article) \n",
        "figure =  plt.figure(figsize = (30, 30))\n",
        "ax =  figure.add_subplot(1, 1, 1)\n",
        "ax.axis('off')\n",
        "ax.imshow(wc)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- stylecloud 패키지로 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IDLpixa430mZ"
      },
      "outputs": [],
      "source": [
        "import stylecloud\n",
        "stylecloud.gen_stylecloud(text = top_nouns_main,\n",
        "                          icon_name=\"fas fa-broadcast-tower\",\n",
        "                          palette=\"colorbrewer.diverging.Spectral_11\",\n",
        "                          font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                          background_color='black',\n",
        "                          gradient=\"horizontal\",\n",
        "                          output_name=\"test.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyF3E1ESuyDQ"
      },
      "source": [
        "### 기사 제목 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LlC2XW2pyMu0"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
        "\n",
        "nouns_title= [ ]\n",
        "for title in articles_title_clean:\n",
        "  tagged = tagger.pos(title)\n",
        "  nouns = [s for s, t in tagged if t in ['NNG', 'NNP', 'VA', 'XR'] and len(s) >1]\n",
        "  for noun in nouns:\n",
        "    if noun not in stop_words:\n",
        "        nouns_title.append(noun)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBXt13JYSANz",
        "outputId": "9c7ed821-d28d-4d0d-ced2-b38a7bce664d"
      },
      "outputs": [],
      "source": [
        "\n",
        "nouns_title_counter = Counter(nouns_title)\n",
        "top_nouns_title = dict(nouns_title_counter.most_common(200))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "2Nhh6tvTv7Nh",
        "outputId": "0d9675f1-22e8-4924-9aa1-4752a7bfa70b"
      },
      "outputs": [],
      "source": [
        "wc.generate_from_frequencies(top_nouns_title) \n",
        "\n",
        "figure =  plt.figure(figsize = (20, 20))\n",
        "ax =  figure.add_subplot(1, 1, 1)\n",
        "ax.axis('off')\n",
        "ax.imshow(wc)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 토픽 모델링 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 토픽 모델링을 위해서 한 기사에서 나온 단어들끼리 리스트로 묶어서 [[],[]  .. ,[]] 형태로 만들어 준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BqZjZod7LqR4"
      },
      "outputs": [],
      "source": [
        "tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
        "\n",
        "\n",
        "# 본문내용\n",
        "nouns_article_lda = []\n",
        "for article in articles_main_clean:\n",
        "  tagged = tagger.pos(article)\n",
        "  nouns = [s for s, t in tagged if t in ['NNG', 'NNP', 'VA', 'XR'] and len(s) >1]\n",
        "  tmp_list = []\n",
        "  for noun in nouns:\n",
        "    if noun not in stop_words:\n",
        "      tmp_list.append(noun)\n",
        "      nouns_article_lda.append(tmp_list)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- gensim 패키지를 활용하여 토픽 모델링 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bFhuRLg9TdHj"
      },
      "outputs": [],
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.callbacks import CoherenceMetric\n",
        "from gensim import corpora\n",
        "from gensim.models.callbacks import PerplexityMetric\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GihHjcOFVmpc"
      },
      "source": [
        "- Bag of words라고 하는 단어 가방을 만들기 위해 고유한 단어들의 사전을 만든다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxdSWS_cTiOu"
      },
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(nouns_article_lda)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 50%이상 나오는 단어, 5번 이하 나오는 단어는 필터링 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52cZz2sUTne9"
      },
      "outputs": [],
      "source": [
        "dictionary.filter_extremes(no_below=5, no_above=0.5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RXfZaWDDVwoA"
      },
      "source": [
        "- 이렇게 사전이 만들어지면 사전 속의 단어가 문장에서 몇 번 출현하는지 빈도를 세서 벡터화. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JvfmhbOnVyIW"
      },
      "outputs": [],
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in nouns_article_lda]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- LDA\n",
        "> 1. 토픽모델링을 몬서를 구성하는 몇 개의 토픽들이 존재하고 있으면 그 토픽들은 단어들의 집합으로 구성되어 있다고 가정  \n",
        "  \n",
        "> 2. 문서 내에 토픽들의 확률분포와 각 토픽을 구성하는 단어들의 확률분포가 주어졌을 때, 문서를 구성하는 토픽을 확률적으로 선택하고   \n",
        "선택된 토픽에 존재하는 단어를 확률적으로 선택하는 샘플링 과정을 반복함으로써 임의의 문서를 생성하는 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kKJUOg2WHCS"
      },
      "outputs": [],
      "source": [
        "#10개 6개 5개 실행 \n",
        "\n",
        "num_topics = 5\n",
        "passes = 20\n",
        "iterations = 400\n",
        "\n",
        "\n",
        "\n",
        "ldamodel = LdaModel(corpus,\n",
        "                    num_topics = num_topics,\n",
        "                    id2word=dictionary, \n",
        "                    iterations= iterations,\n",
        "                    alpha='auto',\n",
        "                    eta='auto',\n",
        "                    eval_every=None,\n",
        "                    passes=passes)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 모델 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_topics = ldamodel.top_topics(corpus) \n",
        "\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Gensim의 토픽모델링 결과를 시각화하는 pyLDAvis 패키지 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_visualization = gensimvis.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.save_html(lda_visualization, 'articles_5.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_topictable_per_doc(ldamodel, corpus):\n",
        "    topic_table = pd.DataFrame()\n",
        "\n",
        "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
        "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
        "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
        "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
        "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
        "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
        "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
        "\n",
        "        # 모든 문서에 대해서 각각 아래를 수행\n",
        "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
        "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
        "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
        "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
        "            else:\n",
        "                break\n",
        "    return(topic_table)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\koreacamel\\AppData\\Local\\Temp\\ipykernel_3236\\1543943919.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>문서 번호</th>\n",
              "      <th>가장 비중이 높은 토픽</th>\n",
              "      <th>가장 높은 토픽의 비중</th>\n",
              "      <th>각 토픽의 비중</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.05251754), (1, 0.05077156), (2, 0.08843...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.05251754), (1, 0.05077266), (2, 0.08843...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517407), (1, 0.050768938), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517407), (1, 0.050770912), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517477), (1, 0.050769866), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517485), (1, 0.050772354), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517448), (1, 0.050771125), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.05251733), (1, 0.05076939), (2, 0.08843...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517418), (1, 0.050771352), (2, 0.088...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>0.4674</td>\n",
              "      <td>[(0, 0.052517418), (1, 0.050770793), (2, 0.088...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
              "0      0             3        0.4674   \n",
              "1      1             3        0.4674   \n",
              "2      2             3        0.4674   \n",
              "3      3             3        0.4674   \n",
              "4      4             3        0.4674   \n",
              "5      5             3        0.4674   \n",
              "6      6             3        0.4674   \n",
              "7      7             3        0.4674   \n",
              "8      8             3        0.4674   \n",
              "9      9             3        0.4674   \n",
              "\n",
              "                                            각 토픽의 비중  \n",
              "0  [(0, 0.05251754), (1, 0.05077156), (2, 0.08843...  \n",
              "1  [(0, 0.05251754), (1, 0.05077266), (2, 0.08843...  \n",
              "2  [(0, 0.052517407), (1, 0.050768938), (2, 0.088...  \n",
              "3  [(0, 0.052517407), (1, 0.050770912), (2, 0.088...  \n",
              "4  [(0, 0.052517477), (1, 0.050769866), (2, 0.088...  \n",
              "5  [(0, 0.052517485), (1, 0.050772354), (2, 0.088...  \n",
              "6  [(0, 0.052517448), (1, 0.050771125), (2, 0.088...  \n",
              "7  [(0, 0.05251733), (1, 0.05076939), (2, 0.08843...  \n",
              "8  [(0, 0.052517418), (1, 0.050771352), (2, 0.088...  \n",
              "9  [(0, 0.052517418), (1, 0.050770793), (2, 0.088...  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
        "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
        "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
        "topictable[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "topictable.to_csv('topictable.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 ('final')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "309bd9f22bf3ce077f0eeb0f13eedd67c8c3b23ce1bee509e5f150b155bd6285"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
